{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\USER\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4924\\85809973.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0minsightface\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0minsightface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFaceAnalysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0minsightface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_image\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mins_get_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\insightface\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'0.7.3'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_zoo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\insightface\\model_zoo\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmodel_zoo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0marcface_onnx\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mArcFaceONNX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mretinaface\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRetinaFace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mscrfd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSCRFD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlandmark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLandmark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\insightface\\model_zoo\\model_zoo.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0marcface_onnx\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mretinaface\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#from .scrfd import *\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\insightface\\model_zoo\\arcface_onnx.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mface_align\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\onnx\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0monnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monnx_cpp2py_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mONNX_ML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m from onnx.external_data_helper import (\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mload_external_data_for_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mwrite_external_data_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\onnx\\external_data_helper.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0monnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monnx_pb\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAttributeProto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGraphProto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModelProto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorProto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\onnx\\onnx_pb.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0monnx_ml_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\onnx\\onnx_ml_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# source: onnx/onnx-ml.proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;34m\"\"\"Generated protocol buffer code.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuilder\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_builder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_descriptor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdescriptor_pool\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_descriptor_pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\USER\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "from deepface.basemodels import VGGFace, OpenFace, Facenet, FbDeepFace\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob \n",
    "import sys\n",
    "import insightface \n",
    "from insightface.app import FaceAnalysis \n",
    "from insightface.data import get_image as ins_get_image \n",
    "import eye_status\n",
    "# from imageio import imread\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Function detectFace is deprecated! Use extract_faces instead of this.\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 0s 316ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "2/2 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 395ms/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "⚠️ Function detectFace is deprecated! Use extract_faces instead of this.\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "2/2 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "⚠️ Function detectFace is deprecated! Use extract_faces instead of this.\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "2/2 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "⚠️ Function detectFace is deprecated! Use extract_faces instead of this.\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "2/2 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "⚠️ Function detectFace is deprecated! Use extract_faces instead of this.\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "distance가 0.3보다 큰경우 0.33487365 0\n",
      "0.3 보다 작은 경우 0.1830863 1\n",
      "distance가 0.3보다 큰경우 0.3899081 1\n",
      "distance가 0.3보다 큰경우 0.33321854 1\n",
      "등록되어 있는 사용자입니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder_name = input('이름을 입력해주세요: ')\n",
    "if not os.path.exists(f'C:/face_detection/{folder_name}'):\n",
    "    folder_path = os.path.join('C:/face_detection/', folder_name)\n",
    "    os.mkdir(folder_path)\n",
    "\n",
    "    print(f'{folder_name} 폴더가 생성되었습니다.')\n",
    "\n",
    "    # 웹캠에 접근하기 위해 VideoCapture 객체 생성\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "    # 타이밍을 지정하여 이미지를 찍고 저장\n",
    "    capture_interval = 5  # 5프레임마다 이미지 찍음\n",
    "    num_captures = 1  # 1개의 이미지\n",
    "    capture_count = 0\n",
    "\n",
    "    while capture_count < num_captures:\n",
    "        # 웹캠에서 프레임을 읽어옵니다.\n",
    "        ret, frame = video_capture.read()\n",
    "\n",
    "        # 프레임이 제대로 읽혔을 경우에만 이미지를 저장합니다.\n",
    "        if ret and capture_count % capture_interval == 0:\n",
    "            # 이미지를 저장할 경로와 파일 이름을 설정합니다.\n",
    "            image_path = f\"{folder_path}/captured_image_{capture_count}.jpg\"\n",
    "\n",
    "            # 프레임을 이미지로 저장합니다.\n",
    "            cv2.imwrite(image_path, frame)\n",
    "\n",
    "            print(f\"이미지 {capture_count+1}가 저장되었습니다.\")\n",
    "            capture_count += 1\n",
    "\n",
    "    # 웹캠 리소스를 해제합니다.\n",
    "    video_capture.release()\n",
    "\n",
    "    options = ['hat', 'mask', 'glasses']\n",
    "    for option in options:\n",
    "        if not input(f'{option} 사진을 찍으시겠습니까?: '):\n",
    "            break\n",
    "        else:\n",
    "            # 웹캠에 접근하기 위해 VideoCapture 객체 생성\n",
    "            video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "            # 타이밍을 지정하여 이미지를 찍고 저장\n",
    "            capture_interval = 5  # 5프레임마다 이미지 찍음\n",
    "            num_captures = 1  # 1개의 이미지\n",
    "            capture_count = 0\n",
    "\n",
    "            while capture_count < num_captures:\n",
    "                # 웹캠에서 프레임을 읽어옵니다.\n",
    "                ret, frame = video_capture.read()\n",
    "\n",
    "                # 프레임이 제대로 읽혔을 경우에만 이미지를 저장합니다.\n",
    "                if ret and capture_count % capture_interval == 0:\n",
    "                    # 이미지를 저장할 경로와 파일 이름을 설정합니다.\n",
    "                    image_path = f\"{folder_path}/captured_image_{option}_{capture_count}.jpg\"\n",
    "\n",
    "                    # 프레임을 이미지로 저장합니다.\n",
    "                    cv2.imwrite(image_path, frame)\n",
    "\n",
    "                    print(f\"이미지 {capture_count+1}가 저장되었습니다.\")\n",
    "                    print(image_path)\n",
    "                    capture_count += 1\n",
    "\n",
    "            # 웹캠 리소스를 해제합니다.\n",
    "            video_capture.release()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "# 인식\n",
    "# 웹캠에 접근하기 위해 VideoCapture 객체 생성\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# 타이밍을 지정하여 이미지를 찍고 저장\n",
    "capture_interval = 5  # 5프레임마다 이미지 찍음\n",
    "num_captures = 1  # 1개의 이미지\n",
    "\n",
    "# 웹캠에서 프레임을 읽어옵니다.\n",
    "ret, frame = video_capture.read()\n",
    "\n",
    "# 프레임이 제대로 읽혔을 경우에만 이미지를 저장합니다.\n",
    "if ret:\n",
    "    img2 = frame\n",
    "\n",
    "# 웹캠 리소스를 해제합니다.\n",
    "video_capture.release()\n",
    "\n",
    "model = VGGFace.loadModel()\n",
    "input_size = model.layers[0].input_shape[1:3]\n",
    "\n",
    "backends = ['opencv', 'ssd', 'dlib', 'mtcnn']\n",
    "\n",
    "img1_list = []\n",
    "distances = []\n",
    "cnt = 0\n",
    "for root, dirs, files in os.walk(folder_name):\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(root, file_name)\n",
    "        img1_list.append(file_path)\n",
    "\n",
    "img2 = DeepFace.detectFace(img2, detector_backend = backends[3])\n",
    "img2 = np.expand_dims(img2, axis=0)\n",
    "img2_representation = model.predict(img2)[0, :]\n",
    "\n",
    "for img in img1_list:\n",
    "    img1 = DeepFace.detectFace(img, detector_backend=backends[3])\n",
    "    img1 = np.expand_dims(img1, axis=0)\n",
    "    img1_representation = model.predict(img1)[0, :]\n",
    "    distance_vector = np.square(img1_representation - img2_representation)\n",
    "    distance = np.sqrt(distance_vector.sum())\n",
    "    distances.append(distance)\n",
    "\n",
    "for distance in distances:\n",
    "    if distance < 0.3:\n",
    "        cnt += 1\n",
    "\n",
    "        print('0.3 보다 작은 경우',distance, cnt)\n",
    "    else:\n",
    "        print('distance가 0.3보다 큰경우', distance, cnt)\n",
    "\n",
    "if cnt >= 0:\n",
    "    print('등록되어 있는 사용자입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프로젝트 목표 눈 감고 있는 것과 뜨고 있는 것을 구분하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FaceAnalysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4924\\1584263718.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mapp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFaceAnalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'buffalo_l'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'FaceAnalysis' is not defined"
     ]
    }
   ],
   "source": [
    "app = FaceAnalysis(name='buffalo_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'face_rec' has no attribute 'init'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4924\\2673977475.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface_recor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen_eyes_detector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleft_eye_detector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mright_eye_detector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_capture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_rec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_rec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_and_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'face_rec' has no attribute 'init'"
     ]
    }
   ],
   "source": [
    "import face_rec\n",
    "from collections import defaultdict\n",
    "import cv2 \n",
    "import os \n",
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "\n",
    "(model, face_recor, open_eyes_detector,left_eye_detector,right_eye_detector, video_capture, images) = face_rec.init()\n",
    "data = face_rec.process_and_encode(images)\n",
    "\n",
    "eyes_detected = defaultdict(str)\n",
    "while True:\n",
    "    frame = face_rec.detect_and_display(model, video_capture, face_recor, open_eyes_detector,left_eye_detector,right_eye_detector, data, eyes_detected)\n",
    "    cv2.imshow(\"Face Liveness Detector\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "video_capture.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
    "\t# grab the dimensions of the frame and then construct a blob\n",
    "\t# from it\n",
    "\t(h, w) = frame.shape[:2]\n",
    "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
    "\t\t(104.0, 177.0, 123.0))\n",
    "\n",
    "\t# pass the blob through the network and obtain the face detections\n",
    "\tfaceNet.setInput(blob)\n",
    "\tdetections = faceNet.forward()\n",
    "\n",
    "\t# initialize our list of faces, their corresponding locations,\n",
    "\t# and the list of predictions from our face mask network\n",
    "\tfaces = []\n",
    "\tlocs = []\n",
    "\tpreds = []\n",
    "\n",
    "\t# loop over the detections\n",
    "\tfor i in range(0, detections.shape[2]):\n",
    "\t\t# extract the confidence (i.e., probability) associated with\n",
    "\t\t# the detection\n",
    "\t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\t\t# filter out weak detections by ensuring the confidence is\n",
    "\t\t# greater than the minimum confidence\n",
    "\t\tif confidence > args[\"confidence\"]:\n",
    "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
    "\t\t\t# the object\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t# ensure the bounding boxes fall within the dimensions of\n",
    "\t\t\t# the frame\n",
    "\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
    "\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "\n",
    "\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
    "\t\t\t# ordering, resize it to 224x224, and preprocess it\n",
    "\t\t\tface = frame[startY:endY, startX:endX]\n",
    "\t\t\tif face.any():\n",
    "\t\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "\t\t\t\tface = cv2.resize(face, (224, 224))\n",
    "\t\t\t\tface = img_to_array(face)\n",
    "\t\t\t\tface = preprocess_input(face)\n",
    "\n",
    "\t\t\t\t# add the face and bounding boxes to their respective\n",
    "\t\t\t\t# lists\n",
    "\t\t\t\tfaces.append(face)\n",
    "\t\t\t\tlocs.append((startX, startY, endX, endY))\n",
    "\n",
    "\t# only make a predictions if at least one face was detected\n",
    "\tif len(faces) > 0:\n",
    "\t\t# for faster inference we'll make batch predictions on *all*\n",
    "\t\t# faces at the same time rather than one-by-one predictions\n",
    "\t\t# in the above `for` loop\n",
    "\t\tfaces = np.array(faces, dtype=\"float32\")\n",
    "\t\tpreds = maskNet.predict(faces, batch_size=32)\n",
    "\n",
    "\t# return a 2-tuple of the face locations and their corresponding\n",
    "\t# locations\n",
    "\treturn (locs, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
